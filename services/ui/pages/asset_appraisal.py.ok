#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¦ Asset Appraisal Agent â€” Full E2E Flow (Inputs â†’ Anonymize â†’ AI â†’ Human Review â†’ Training)
Author:  Nguyen Dzoan
Version: 2025-11-01

Includes:
- Stage 1: CSV + evidence (images/PDFs) + manual row; synthetic fallback + "why" table
- Stage 2: Explicit anonymization pipeline (RAW & ANON kept)
- Stage 3: AI appraisal with runtime flavor selector, agent discovery+probe, rule_reasons when backend omits
  + Production banner + asset-trained model selector + promote inside Stage 3
- Stage 4: Human Review with AIâ†”Human agreement gauge; export feedback CSV
- Stage 5: Training (upload feedback) â†’ Train candidate â†’ Promote to PRODUCTION
"""

import os, io, re, json, datetime, requests
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PAGE CONFIG + SIDEBAR HIDE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Asset Appraisal Agent", layout="wide")
st.markdown("""
<style>
[data-testid="stSidebar"], section[data-testid="stSidebar"], nav[data-testid="stSidebarNav"] { display:none !important; }
[data-testid="stAppViewContainer"] { margin-left:0 !important; padding-left:0 !important; }
div[data-testid="stMetricValue"] { color:#38bdf8 !important; }
</style>
""", unsafe_allow_html=True)

API_URL = os.getenv("API_URL", "http://localhost:8090")

# Default fallbacks (will be superseded by discovery)
ASSET_AGENT_IDS = [a.strip() for a in os.getenv("ASSET_AGENT_IDS", "asset_appraisal,asset").split(",") if a.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NAV
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_nav_bar_app():
    stage = st.session_state.get("asset_stage", "login")
    show_home   = stage in ("agents", "credit_agent", "asset_agent", "login", "asset_flow")
    show_agents = stage not in ("landing", "agents")
    if not (show_home or show_agents): return
    c1, c2, _ = st.columns([1,1,6])
    with c1:
        if show_home and st.button("ğŸ  Back to Home", key=f"btn_home_asset_{stage}"):
            st.session_state["asset_stage"] = "landing"; st.rerun()
    with c2:
        if show_agents and st.button("ğŸ¤– Back to Agents", key=f"btn_agents_asset_{stage}"):
            st.session_state["asset_stage"] = "agents"; st.rerun()
    st.markdown("---")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SESSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ss = st.session_state
ss.setdefault("asset_stage", "login")
ss.setdefault("asset_logged_in", False)
ss.setdefault("asset_user", None)

# Stage caches
ss.setdefault("asset_raw_df", None)     # Stage 1 raw (after CSV/manual merge)
ss.setdefault("asset_evidence", [])     # evidence filenames (images/pdfs)
ss.setdefault("asset_anon_df", None)    # Stage 2 anonymized
ss.setdefault("asset_stage2_df", None)  # Stage 3 input (resolved source)
ss.setdefault("asset_ai_df", None)      # Stage 3 AI output
ss.setdefault("asset_selected_model", None)  # trained model path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def anonymize_text_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for col in out.columns:
        if out[col].dtype == "object":
            out[col] = (
                out[col].astype(str)
                .apply(lambda x: re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "[EMAIL]", x))
            )
    return out

def quick_synth(rows: int = 150) -> pd.DataFrame:
    """Generate asset rows + finance metrics for demo/backup."""
    rng = np.random.default_rng(42)
    cities = [
        ("Hanoi", 21.0285, 105.8542),
        ("HCMC", 10.7769, 106.7009),
        ("Da Nang", 16.0544, 108.2022),
        ("Hue", 16.4637, 107.5909),
        ("Can Tho", 10.0452, 105.7469),
    ]
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, rows + 1)],
        "asset_id": [f"A{i:04d}" for i in range(1, rows + 1)],
        "asset_type": rng.choice(["House","Apartment","Car","Land","Factory"], rows),
        "age_years": rng.integers(1, 40, rows),
        "market_value": rng.integers(50_000, 2_000_000, rows),
        "condition_score": rng.uniform(0.6, 1.0, rows),
        "legal_penalty": rng.uniform(0.95, 1.0, rows),          # legal/title risk adj
        "employment_years": rng.integers(0, 30, rows),
        "credit_history_years": rng.integers(0, 25, rows),
        "delinquencies": rng.integers(0, 6, rows),
        "current_loans": rng.integers(0, 8, rows),
        "loan_amount": rng.integers(10_000, 200_000, rows),
        "customer_type": rng.choice(["bank","non-bank"], rows, p=[0.7,0.3]),
    })
    cdf = pd.DataFrame(cities, columns=["city","lat","lon"])
    df["city"] = rng.choice(cdf["city"], rows)
    df = df.merge(cdf, on="city", how="left")
    df["depreciation_rate"] = (1 - df["condition_score"]) * 100
    df["market_segment"] = np.where(df["market_value"] > 500_000, "High", "Mass")
    df["DTI"] = rng.uniform(0.05, 0.9, rows)
    df["LTV"] = np.clip(df["loan_amount"] / np.maximum(df["market_value"], 1), 0.05, 1.5)
    df["evidence_files"] = [[] for _ in range(rows)]
    return df

def synth_why_table() -> pd.DataFrame:
    return pd.DataFrame([
        {"Metric": "DTI", "Why": "Debt service relative to income â€” proxy for payability."},
        {"Metric": "LTV", "Why": "Loan vs asset value â€” proxy for collateral adequacy."},
        {"Metric": "condition_score", "Why": "Asset physical state impacts fair value/depreciation."},
        {"Metric": "legal_penalty", "Why": "Legal/title flags reduce realizable value."},
        {"Metric": "employment_years / credit_history_years", "Why": "Stability/track record."},
        {"Metric": "delinquencies / current_loans", "Why": "Current risk pressure."},
        {"Metric": "market_segment / city / lat,lon", "Why": "Market & location effects on pricing."},
    ])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AGENT DISCOVERY & PROBE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_get_json(url: str, timeout: int = 8):
    try:
        r = requests.get(url, timeout=timeout)
        if r.ok:
            try:
                return True, r.json()
            except Exception as e:
                return False, f"parse error: {e}\nBody:\n{r.text[:2000]}"
        return False, f"{r.status_code} {r.reason}\nBody:\n{r.text[:2000]}"
    except Exception as e:
        return False, f"request error: {e}"

def discover_asset_agents() -> list[str]:
    """Try common discovery endpoints and extract agent ids. Cache in session."""
    cached = st.session_state.get("asset_agent_ids")
    if isinstance(cached, list) and cached:
        return cached

    candidates = []

    # 1) /v1/agents (prefer)
    ok, data = _safe_get_json(f"{API_URL}/v1/agents")
    if ok:
        try:
            if isinstance(data, dict) and "agents" in data:
                items = data["agents"]
                if isinstance(items, list):
                    for it in items:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name") or it.get("agent") or it.get("slug")
                            if aid: candidates.append(aid)
            elif isinstance(data, list):
                for it in data:
                    if isinstance(it, str):
                        candidates.append(it)
                    elif isinstance(it, dict):
                        aid = it.get("id") or it.get("name")
                        if aid: candidates.append(aid)
        except Exception:
            pass

    # 2) /v1/agents/list (alt)
    if not candidates:
        ok2, data2 = _safe_get_json(f"{API_URL}/v1/agents/list")
        if ok2:
            try:
                if isinstance(data2, dict):
                    for k in ("agents", "data", "items"):
                        if k in data2 and isinstance(data2[k], list):
                            for it in data2[k]:
                                if isinstance(it, str):
                                    candidates.append(it)
                                elif isinstance(it, dict):
                                    aid = it.get("id") or it.get("name")
                                    if aid: candidates.append(aid)
                elif isinstance(data2, list):
                    for it in data2:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)
            except Exception:
                pass

    # 3) /v1/health (sometimes lists agents)
    if not candidates:
        ok3, data3 = _safe_get_json(f"{API_URL}/v1/health")
        if ok3 and isinstance(data3, dict):
            for k in ("agents", "services", "available_agents"):
                val = data3.get(k)
                if isinstance(val, list):
                    for it in val:
                        if isinstance(it, str):
                            candidates.append(it)
                        elif isinstance(it, dict):
                            aid = it.get("id") or it.get("name")
                            if aid: candidates.append(aid)

    discovered = [c for c in dict.fromkeys(candidates) if c]  # de-dupe
    if not discovered:
        discovered = ASSET_AGENT_IDS[:]  # fallback to env/defaults

    st.session_state["asset_agent_ids"] = discovered
    return discovered

def probe_api() -> dict:
    """Collect quick diagnostics for UI."""
    diag = {}
    for path in ("/v1/health", "/v1/agents", "/v1/agents/list"):
        ok, data = _safe_get_json(f"{API_URL}{path}")
        diag[path] = data if ok else {"error": data}
    diag["API_URL"] = API_URL
    diag["discovered_agents"] = discover_asset_agents()
    return diag

# NEW: run_id extractor for various API payload shapes
def _extract_run_id(obj) -> str | None:
    """Find a run_id in a nested dict/list API response."""
    if isinstance(obj, dict):
        rid = obj.get("run_id")
        if isinstance(rid, str) and rid:
            return rid
        for k in ("data", "meta", "result", "payload"):
            v = obj.get(k)
            if isinstance(v, dict):
                rid = v.get("run_id")
                if isinstance(rid, str) and rid:
                    return rid
    elif isinstance(obj, list):
        for it in obj:
            rid = _extract_run_id(it)
            if rid:
                return rid
    return None

def try_run_asset_agent(csv_bytes: bytes, form_fields: dict, timeout_sec: int = 180):
    """
    Discover agent ids, then try each. Rebuild multipart for each attempt.
    Preferred: use run_id to GET merged CSV and DataFrame it.
    Fallback: normalize 'result' only (not whole JSON).

    Returns (ok: bool, DataFrame | error_string)
    """
    agent_ids = discover_asset_agents()
    errors = []
    for agent_id in agent_ids:
        files = {"file": ("asset_verified.csv", io.BytesIO(csv_bytes), "text/csv")}
        url = f"{API_URL}/v1/agents/{agent_id}/run"
        try:
            resp = requests.post(url, files=files, data=form_fields, timeout=timeout_sec)
        except Exception as e:
            errors.append(f"[{agent_id}] request error: {e}")
            continue

        if resp.ok:
            body_text = resp.text[:4000]
            try:
                payload = resp.json()
            except Exception as e:
                errors.append(f"[{agent_id}] parse error: {e}\nBody:\n{body_text}")
                continue

            rid = _extract_run_id(payload)
            if rid:
                # Preferred: fetch merged CSV
                try:
                    r_csv = requests.get(f"{API_URL}/v1/runs/{rid}/report?format=csv", timeout=60)
                    if r_csv.ok:
                        df = pd.read_csv(io.BytesIO(r_csv.content))
                        st.session_state["asset_last_run_id"] = rid
                        st.session_state["asset_last_runner"] = ((payload.get("meta") or {}).get("runner_used"))
                        return True, df
                    else:
                        errors.append(
                            f"[{agent_id}] report GET {r_csv.status_code} {r_csv.reason} for run_id={rid}\n"
                            f"Body:\n{r_csv.text[:2000]}"
                        )
                except Exception as e:
                    errors.append(f"[{agent_id}] report GET error for run_id={rid}: {e}")

            # Fallback: try to render just 'result'
            result_part = payload.get("result")
            if isinstance(result_part, list):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            elif isinstance(result_part, dict):
                try:
                    df = pd.json_normalize(result_part)
                    return True, df
                except Exception as e:
                    errors.append(f"[{agent_id}] fallback normalize error: {e}\nBody:\n{body_text}")
            else:
                errors.append(f"[{agent_id}] no run_id and empty/unknown 'result'.\nBody:\n{body_text}")
        else:
            errors.append(f"[{agent_id}] {resp.status_code} {resp.reason}\nBody:\n{resp.text[:2000]}")

    return False, "All agent attempts failed (discovered=" + ", ".join(agent_ids) + "):\n" + "\n\n".join(errors)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LOGIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ss["asset_stage"] == "login" and not ss["asset_logged_in"]:
    render_nav_bar_app()
    st.title("ğŸ” Login to AI Asset Appraisal Platform")
    c1, c2, c3 = st.columns([1,1,1])
    with c1: user = st.text_input("Username", placeholder="e.g. dzoan")
    with c2: email = st.text_input("Email", placeholder="e.g. dzoan@demo.local")
    with c3: pwd = st.text_input("Password", type="password", placeholder="Enter any password")
    if st.button("Login", key="btn_asset_login", use_container_width=True):
        if user.strip() and email.strip():
            ss["asset_user"] = {"name": user.strip(), "email": email.strip(), "timestamp": datetime.datetime.utcnow().isoformat()}
            ss["asset_logged_in"] = True
            ss["asset_stage"] = "asset_flow"
            st.rerun()
        else:
            st.error("âš ï¸ Please fill all fields before continuing.")
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WORKFLOW
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ss["asset_logged_in"]:
    render_nav_bar_app()
    st.title("ğŸ›ï¸ Asset Appraisal Agent")
    st.caption(f"E2E flow â€” Inputs â†’ Anonymize â†’ AI â†’ Human Review â†’ Training | ğŸ‘‹ {ss['asset_user']['name']}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TABS (1..5)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“¥ 1) Data Input",
        "ğŸ§¹ 2) Anonymize",
        "ğŸ¤– 3) AI Appraisal & Valuation",
        "ğŸ§‘â€âš–ï¸ 4) Human Review",
        "ğŸ§ª 5) Training (Feedback â†’ Retrain)"
    ])

    # ========== 1) DATA INPUT ==========
    with tab1:
        st.subheader("ğŸ“¥ Stage 1 â€” Provide Asset Data (CSV, evidence files, or manual)")
        left, right = st.columns([1.4, 1])

        with left:
            up_csv = st.file_uploader("Upload Asset CSV", type=["csv"], key="asset_csv")
            evid = st.file_uploader("Attach evidence (images or PDFs, optional)", type=["png","jpg","jpeg","pdf"],
                                    accept_multiple_files=True, key="asset_evidence")
            with st.expander("â• Add manual asset row", expanded=False):
                m1, m2 = st.columns(2)
                asset_type = m1.selectbox("Asset Type", ["House","Apartment","Car","Land","Factory"])
                market_value = m2.number_input("Market Value ($)", 0, 10_000_000, 250_000, step=1_000)
                age_years = m1.number_input("Age (years)", 0, 100, 10)
                loan_amount = m2.number_input("Requested Loan ($)", 0, 10_000_000, 120_000, step=1_000)
                employment_years = m1.number_input("Employment Years", 0, 60, 5)
                credit_hist_years = m2.number_input("Credit History (years)", 0, 50, 6)
                delinq = m1.number_input("Delinquencies", 0, 50, 1)
                curr_loans = m2.number_input("Current Loans", 0, 50, 2)
                city = m1.text_input("City (optional)", "HCMC")
                add_row = st.button("Add manual asset row", key="btn_add_manual_row")

            if st.button("Build Stage 1 dataset (or fallback to synthetic if empty)", key="btn_build_stage1"):
                rows = []
                if up_csv is not None:
                    try:
                        rows.append(pd.read_csv(up_csv))
                    except Exception as e:
                        st.error(f"CSV parse error: {e}")

                if evid:
                    ss["asset_evidence"] = [f.name for f in evid]

                if add_row:
                    rows.append(pd.DataFrame([{
                        "application_id": f"APP_{datetime.datetime.utcnow().strftime('%H%M%S')}",
                        "asset_id": f"A{datetime.datetime.utcnow().strftime('%M%S')}",
                        "asset_type": asset_type, "market_value": market_value, "age_years": age_years,
                        "loan_amount": loan_amount, "employment_years": employment_years,
                        "credit_history_years": credit_hist_years, "delinquencies": delinq,
                        "current_loans": curr_loans, "city": city
                    }]))

                if len(rows) == 0:
                    df = quick_synth(150)
                    st.info("No inputs provided â€” generated synthetic dataset.")
                    st.dataframe(synth_why_table(), use_container_width=True)
                else:
                    df = pd.concat(rows, ignore_index=True)

                if "evidence_files" not in df.columns:
                    df["evidence_files"] = [ss.get("asset_evidence", []) for _ in range(len(df))]

                ss["asset_raw_df"] = df
                st.success(f"Stage 1 dataset ready. Rows: {len(df)}")
                st.dataframe(df.head(15), use_container_width=True)

        with right:
            st.markdown("#### Generated Metrics â€” What & Why")
            st.dataframe(synth_why_table(), use_container_width=True)

    # ========== 2) ANONYMIZE ==========
    with tab2:
        st.subheader("ğŸ§¹ Stage 2 â€” Anonymize / Sanitize PII")
        if ss["asset_raw_df"] is None:
            st.warning("Build Stage 1 dataset first (tab 1).")
        else:
            if st.button("Run anonymization now", key="btn_run_anon"):
                ss["asset_anon_df"] = anonymize_text_cols(ss["asset_raw_df"])
                st.success("Anonymization complete. Saved ANON dataset.")
            if ss["asset_anon_df"] is not None:
                st.dataframe(ss["asset_anon_df"].head(15), use_container_width=True)
                st.download_button("â¬‡ï¸ Download anonymized CSV",
                                   data=ss["asset_anon_df"].to_csv(index=False).encode("utf-8"),
                                   file_name="asset_anonymized.csv", mime="text/csv")

    # ========== 3) AI APPRAISAL & VALUATION ==========
    with tab3:
        st.subheader("ğŸ¤– Stage 3 â€” AI Appraisal & Valuation")

        # Production model banner (asset agent)
        try:
            resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
            if resp.status_code == 200:
                meta = resp.json()
                if meta.get("has_production"):
                    ver = (meta.get("meta") or {}).get("version", "1.x")
                    src = (meta.get("meta") or {}).get("source", "production")
                    st.success(f"ğŸŸ¢ Production model active â€” version: {ver} â€¢ source: {src}")
                else:
                    st.warning("âš ï¸ No production model promoted yet â€” using baseline.")
            else:
                st.info("â„¹ï¸ Could not fetch production model meta.")
        except Exception:
            st.info("â„¹ï¸ Production meta unavailable.")

        
        
        
        # # ğŸ§© Model Selection (asset trained models) â€” sort by raw ctime (numeric)
        # trained_dir = os.path.expanduser(
        #     "~/credit-appraisal-agent-poc/agents/asset_appraisal/models/trained"
        # )
        
        # models = []
        # if os.path.exists(trained_dir):
        #     for f in os.listdir(trained_dir):
        #         if f.endswith(".joblib"):
        #             fpath = os.path.join(trained_dir, f)
        #             ctime = os.path.getctime(fpath)
        #             created_str = datetime.datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
        #             models.append((f, fpath, ctime, created_str))

        # if models:
        #     models.sort(key=lambda x: x[2], reverse=True)  # newest by ctime
        #     display_names = [f"{m[0]} â€” {m[3]}" for m in models]
        #     selected_display = st.selectbox("ğŸ“¦ Select trained model to use", display_names, key="asset_model_select")
        #     selected_model = models[display_names.index(selected_display)][1]
        #     st.success(f"âœ… Using model: {os.path.basename(selected_model)}")
        #     st.session_state["asset_selected_model"] = selected_model

        #     if st.button("ğŸš€ Promote this model to Production", key="asset_promote_model"):
        #         try:
        #             prod_path = os.path.expanduser(
        #                 "~/credit-appraisal-agent-poc/agents/asset_appraisal/models/production/model.joblib"
        #             )
        #             os.makedirs(os.path.dirname(prod_path), exist_ok=True)
        #             import shutil
        #             shutil.copy2(selected_model, prod_path)
        #             st.success(f"âœ… Model promoted to production: {os.path.basename(prod_path)}")
        #         except Exception as e:
        #             st.error(f"âŒ Promotion failed: {e}")
        # else:
        #     st.warning("âš ï¸ No trained models found â€” train one in Step 5 first.")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ§© Model Selection (asset trained + production)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        trained_dir = os.path.expanduser(
            "~/credit-appraisal-agent-poc/agents/asset_appraisal/models/trained"
        )
        production_default_fp = os.path.expanduser(
            "~/credit-appraisal-agent-poc/agents/asset_appraisal/models/production/model.joblib"
        )

        # Try to fetch production meta (compatible with your asset training router)
        production_fp = None
        try:
            r = requests.get(
                f"{API_URL}/v1/training/production_meta",
                params={"agent_id": "asset_appraisal"},
                timeout=5
            )
            if r.ok:
                j = r.json() or {}
                if j.get("has_production"):
                    meta = j.get("meta") or {}
                    production_fp = (
                        meta.get("model_path")
                        or meta.get("promoted_to")
                        or production_default_fp
                    )
        except Exception:
            production_fp = None

        if not production_fp:
            production_fp = production_default_fp

        # Build models list: (label, path, ctime, created_str, is_production)
        models = []

        # 1) Add PRODUCTION if present
        if os.path.exists(production_fp):
            try:
                p_ctime = os.path.getctime(production_fp)
                p_created = datetime.datetime.fromtimestamp(p_ctime).strftime("%b %d, %Y %H:%M")
            except Exception:
                p_ctime, p_created = 0.0, "production"
            models.append(("â­ Production", production_fp, p_ctime, p_created, True))

        # 2) Add TRAINED models
        if os.path.exists(trained_dir):
            for f in os.listdir(trained_dir):
                if not f.endswith(".joblib"):
                    continue
                fpath = os.path.join(trained_dir, f)
                try:
                    ctime = os.path.getctime(fpath)
                    created_str = datetime.datetime.fromtimestamp(ctime).strftime("%b %d, %Y %H:%M")
                except Exception:
                    ctime, created_str = 0.0, ""
                # skip if literally same file as production
                try:
                    if os.path.exists(production_fp) and os.path.samefile(fpath, production_fp):
                        continue
                except Exception:
                    pass
                models.append((f, fpath, ctime, created_str, False))

        # Sort: production first, then newest trained
        models = sorted(models, key=lambda x: (0 if x[4] else 1, -x[2]))

        if models:
            display_names = [
                f"{label} â€” {created}" if created else f"{label}"
                for (label, _, _, created, _) in models
            ]

            # keep previous selection if present
            default_idx = 0
            prev_selected = st.session_state.get("asset_selected_model")
            if prev_selected:
                for i, (_, path, _, _, _) in enumerate(models):
                    if path == prev_selected:
                        default_idx = i
                        break

            selected_display = st.selectbox(
                "ğŸ“¦ Select trained model to use",
                display_names,
                index=default_idx,
                key="asset_model_select"
            )
            sel_idx = display_names.index(selected_display)
            selected_model = models[sel_idx][1]
            is_prod = models[sel_idx][4]

            st.session_state["asset_selected_model"] = selected_model

            if is_prod:
                st.success(f"ğŸŸ¢ Using PRODUCTION model: {os.path.basename(selected_model)}")
            else:
                st.success(f"âœ… Using model: {os.path.basename(selected_model)}")

            # Promote only when a non-production model is selected
            if not is_prod and st.button("ğŸš€ Promote this model to Production", key="asset_promote_model"):
                try:
                    # backend promotes newest trained; aligns with your training router
                    r = requests.post(
                        f"{API_URL}/v1/agents/asset_appraisal/training/promote_last", timeout=60
                    )
                    if r.ok:
                        st.success("âœ… Model promoted to PRODUCTION.")
                        st.rerun()  # refresh so â­ Production appears and becomes selected
                    else:
                        try:
                            st.error(f"âŒ Promotion failed: {r.status_code} {r.reason}")
                            st.code(r.json())
                        except Exception:
                            st.code(r.text[:2000])
                except Exception as e:
                    st.error(f"âŒ Promotion error: {e}")
        else:
            st.warning("âš ï¸ No trained models found â€” train one in Step 5 first.")


        # ğŸ§  Local LLM & Hardware Profile (runtime hints)
        LLM_MODELS = [
            ("Phi-3 Mini (3.8B) â€” CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
            ("Mistral 7B Instruct â€” CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU â‰¥8GB"),
            ("Gemma-2 7B â€” CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU â‰¥8GB"),
            ("LLaMA-3 8B â€” GPU recommended", "llama3:8b-instruct", "GPU â‰¥12GB (CPU very slow)"),
            ("Qwen2 7B â€” GPU recommended", "qwen2:7b-instruct", "GPU â‰¥12GB (CPU very slow)"),
            ("Mixtral 8x7B â€” GPU only (big)", "mixtral:8x7b-instruct", "GPU 24â€“48GB"),
        ]
        LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
        LLM_VALUE_BY_LABEL = {l: v for (l, v, _) in LLM_MODELS}
        LLM_HINT_BY_LABEL  = {l: h for (l, _, h) in LLM_MODELS}

        OPENSTACK_FLAVORS = {
            "m4.medium":  "4 vCPU / 8 GB RAM â€” CPU-only small",
            "m8.large":   "8 vCPU / 16 GB RAM â€” CPU-only medium",
            "g1.a10.1":   "8 vCPU / 32 GB RAM + 1Ã—A10 24GB",
            "g1.l40.1":   "16 vCPU / 64 GB RAM + 1Ã—L40 48GB",
            "g2.a100.1":  "24 vCPU / 128 GB RAM + 1Ã—A100 80GB",
        }

        with st.expander("ğŸ§  Local LLM & Hardware Profile", expanded=True):
            c1, c2 = st.columns([1.2, 1])
            with c1:
                model_label = st.selectbox("Local LLM (used for narratives/explanations)", LLM_LABELS, index=1, key="asset_llm_label")
                llm_value = LLM_VALUE_BY_LABEL[model_label]
                use_llm = st.checkbox("Use LLM narrative (include explanations)", value=False, key="asset_use_llm")
                st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
            with c2:
                flavor = st.selectbox("OpenStack flavor / host profile", list(OPENSTACK_FLAVORS.keys()), index=0, key="asset_flavor")
                st.caption(OPENSTACK_FLAVORS[flavor])
            st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

        # Choose data source for run
        src = st.selectbox("Data source for AI run", [
            "Use ANON (from Stage 2)",
            "Use RAW â†’ auto-sanitize",
            "Use synthetic (fallback)",
        ])

        if src == "Use ANON (from Stage 2)":
            df2 = ss.get("asset_anon_df")
        elif src == "Use RAW â†’ auto-sanitize":
            df2 = anonymize_text_cols(ss.get("asset_raw_df")) if ss.get("asset_raw_df") is not None else None
        else:
            df2 = quick_synth(150)

        if df2 is None:
            st.warning("No dataset available. Build Stage 1 & run anonymization first.")
            st.stop()

        st.dataframe(df2.head(10), use_container_width=True)

        # ğŸ”§ Policy & Haircut Controls (asset-centric)
        st.markdown("### âš™ï¸ Policy & Haircut Controls")
        p1, p2, p3 = st.columns([1, 1, 1])

        with p1:
            min_confidence = st.slider("Min confidence to auto-approve (%)", 0, 100, 70, 1)
            base_haircut   = st.slider("Base haircut (all assets, %)", 0, 50, 5, 1)
        with p2:
            legal_floor    = st.slider("Legal quality floor (min legal_penalty)", 0.90, 1.00, 0.97, 0.01)
            condition_floor= st.slider("Condition floor (min condition_score)", 0.60, 1.00, 0.75, 0.01)
        with p3:
            ltv_cap_mode   = st.selectbox("LTV cap mode", ["Fixed cap", "Per asset_type"], index=0)
            fixed_ltv_cap  = st.slider("Fixed LTV cap (Ã—)", 0.10, 2.00, 0.80, 0.05)

        # Per-type caps if requested
        type_caps = {}
        if ltv_cap_mode == "Per asset_type":
            types = sorted(list(map(str, (df2.get("asset_type") or pd.Series(["Asset"])).dropna().unique())))[0:8]
            st.caption("Tune LTV caps per asset_type")
            grid_cols = st.columns(4 if len(types) > 3 else max(1, len(types)))
            for idx, t in enumerate(types):
                with grid_cols[idx % len(grid_cols)]:
                    type_caps[t] = st.number_input(f"{t} LTV cap Ã—", 0.10, 2.00, 0.80, 0.05, key=f"cap_{t}")

        # Probe API (health & agents)
        with st.expander("ğŸ” Probe API (health & agents)", expanded=False):
            if st.button("Run probe now", key="btn_probe_api"):
                diag = probe_api()
                st.json(diag)

        # Run model button (runtime flavor included)
        if st.button("ğŸš€ Run AI Appraisal now", key="btn_run_ai"):
            csv_bytes = df2.to_csv(index=False).encode("utf-8")

            form_fields = {
                "use_llm": str(use_llm).lower(),
                "llm": llm_value,
                "flavor": flavor,
                "selected_model": ss.get("asset_selected_model", ""),
                "agent_name": "asset_appraisal",
                # Policy hints (backend may ignore; UI still enforces locally)
                "min_confidence": str(min_confidence),
                "legal_floor": str(legal_floor),
                "condition_floor": str(condition_floor),
                "ltv_cap_mode": ltv_cap_mode,
                "fixed_ltv_cap": str(fixed_ltv_cap),
                "type_caps": json.dumps(type_caps),
                "base_haircut": str(base_haircut),
            }

            with st.spinner("Calling asset agentâ€¦"):
                ok, result = try_run_asset_agent(csv_bytes, form_fields=form_fields, timeout_sec=180)

            if not ok:
                st.error("âŒ Model API error.")
                st.info("Tip: open 'ğŸ” Probe API' above to see health and discovered agent ids.")
                st.code(str(result)[:8000])
                st.stop()

            df_app = result.copy()

            # Ensure core columns
            if "ai_adjusted" not in df_app.columns and "market_value" in df_app.columns:
                df_app["ai_adjusted"] = df_app["market_value"]
            if "confidence" not in df_app.columns:
                df_app["confidence"] = 80.0
            if "legal_penalty" not in df_app.columns:
                df_app["legal_penalty"] = 1.0
            if "condition_score" not in df_app.columns:
                df_app["condition_score"] = 0.9
            if "loan_amount" not in df_app.columns:
                df_app["loan_amount"] = 0.0

            # Compute realizable value after haircuts
            df_app["realizable_value"] = (
                pd.to_numeric(df_app["ai_adjusted"], errors="coerce") *
                pd.to_numeric(df_app["legal_penalty"], errors="coerce") *
                pd.to_numeric(df_app["condition_score"], errors="coerce") *
                (1.0 - float(base_haircut) / 100.0)
            )

            # LTV (AI) and valuation gap %
            mv = pd.to_numeric(df_app.get("market_value", np.nan), errors="coerce")
            ai = pd.to_numeric(df_app.get("ai_adjusted", np.nan), errors="coerce")
            la = pd.to_numeric(df_app.get("loan_amount", np.nan), errors="coerce")

            df_app["valuation_gap_pct"] = (ai - mv) / mv.replace(0, np.nan) * 100.0
            df_app["ltv_ai"] = la / ai.replace(0, np.nan)

            # Determine LTV caps row-by-row
            if ltv_cap_mode == "Fixed cap":
                df_app["ltv_cap"] = float(fixed_ltv_cap)
            else:
                atypes = df_app.get("asset_type").astype(str) if "asset_type" in df_app.columns else pd.Series(["Asset"] * len(df_app))
                df_app["ltv_cap"] = atypes.map(lambda t: float(type_caps.get(t, fixed_ltv_cap)))

            # Policy breaches & decision
            breaches = []
            conf = pd.to_numeric(df_app["confidence"], errors="coerce")
            legal = pd.to_numeric(df_app["legal_penalty"], errors="coerce")
            cond  = pd.to_numeric(df_app["condition_score"], errors="coerce")
            ltv   = pd.to_numeric(df_app["ltv_ai"], errors="coerce")
            lcap  = pd.to_numeric(df_app["ltv_cap"], errors="coerce")

            for i in range(len(df_app)):
                b = []
                if pd.notna(conf.iat[i]) and conf.iat[i] < min_confidence:
                    b.append(f"confidence<{min_confidence}%")
                if pd.notna(legal.iat[i]) and legal.iat[i] < legal_floor:
                    b.append(f"legal<{legal_floor:.2f}")
                if pd.notna(cond.iat[i]) and cond.iat[i] < condition_floor:
                    b.append(f"condition<{condition_floor:.2f}")
                if pd.notna(ltv.iat[i]) and pd.notna(lcap.iat[i]) and ltv.iat[i] > lcap.iat[i]:
                    b.append("ltv>cap")
                breaches.append(", ".join(b))

            df_app["policy_breaches"] = breaches
            df_app["decision"] = np.where(df_app["policy_breaches"].str.len().gt(0), "review", "approved")

            # First Table (loan-centric validation)
            cols_first = [c for c in [
                "application_id","asset_id","asset_type","city",
                "market_value","ai_adjusted","realizable_value",
                "loan_amount","ltv_ai","ltv_cap",
                "confidence","legal_penalty","condition_score",
                "valuation_gap_pct","policy_breaches","decision"
            ] if c in df_app.columns]
            first_table = df_app[cols_first].copy()

            ss["asset_ai_df"] = df_app
            ss["asset_first_table"] = first_table

            st.success("âœ… AI appraisal completed.")
            st.markdown("### ğŸ§¾ Loan & Asset Validation â€” First Table")
            st.dataframe(first_table, use_container_width=True)
            st.download_button(
                "â¬‡ï¸ Export First Table (CSV)",
                data=first_table.to_csv(index=False).encode("utf-8"),
                file_name="asset_appraisal_first_table.csv",
                mime="text/csv"
            )

        # ğŸ“Š Portfolio Insights Dashboard
        st.divider()
        st.subheader("ğŸ“Š Portfolio Insights Dashboard")

        ft = ss.get("asset_first_table")
        if ft is None or (hasattr(ft, "empty") and ft.empty):
            st.info("Run appraisal to populate the dashboard.")
        else:
            # Safe numerics & copy
            ft = ft.copy()
            def _num(s): return pd.to_numeric(s, errors="coerce")
            for c in ["ai_adjusted","realizable_value","loan_amount",
                      "valuation_gap_pct","ltv_ai","ltv_cap","confidence"]:
                if c in ft.columns:
                    ft[c] = _num(ft[c])

            # KPI strip
            k1, k2, k3, k4 = st.columns(4)
            if {"ltv_ai","ltv_cap"}.issubset(ft.columns):
                breach = (ft["ltv_ai"] > ft["ltv_cap"])
                breach_rate = float(breach.mean() * 100)
            else:
                breach_rate = 0.0
            total_ai        = float(_num(ft.get("ai_adjusted", pd.Series(dtype=float))).sum())
            total_realiz    = float(_num(ft.get("realizable_value", pd.Series(dtype=float))).sum())
            avg_gap         = float(_num(ft.get("valuation_gap_pct", pd.Series(dtype=float))).mean())
            avg_conf        = float(_num(ft.get("confidence", pd.Series(dtype=float))).mean())
            k1.metric("Breach Rate (LTV>cap)", f"{breach_rate:.1f}%")
            k2.metric("AI Gross Value",       f"${total_ai:,.0f}")
            k3.metric("Realizable Value",     f"${total_realiz:,.0f}")
            k4.metric("Avg Valuation Gap",    f"{avg_gap:+.2f}%")

            # Row 1: Decision mix & Gap histogram
            r1c1, r1c2 = st.columns(2)
            with r1c1:
                try:
                    names_series = (ft["decision"].astype(str).str.title()
                                    if "decision" in ft.columns
                                    else np.where(ft.get("policy_breaches","").astype(str).str.len().gt(0),
                                                  "Has Breach","No Breach"))
                    fig_mix = px.pie(ft, names=names_series, title="Decision / Breach Mix")
                    fig_mix.update_layout(template="plotly_dark", height=320)
                    st.plotly_chart(fig_mix, use_container_width=True)
                except Exception:
                    pass
            with r1c2:
                if "valuation_gap_pct" in ft.columns:
                    try:
                        fig_gap = px.histogram(ft, x="valuation_gap_pct", nbins=40, title="Valuation Gap % Distribution")
                        fig_gap.update_layout(template="plotly_dark", height=320)
                        st.plotly_chart(fig_gap, use_container_width=True)
                    except Exception:
                        pass

            # Row 2: LTV vs Cap & City concentration
            r2c1, r2c2 = st.columns(2)
            with r2c1:
                if {"ltv_ai","ltv_cap"}.issubset(ft.columns):
                    try:
                        fig_sc = px.scatter(
                            ft, x="ltv_cap", y="ltv_ai",
                            hover_data=[c for c in ["application_id","asset_id","asset_type","city"] if c in ft.columns],
                            title="LTV (AI) vs LTV Cap"
                        )
                        max_cap = float((ft["ltv_cap"].max() or 1.2))
                        fig_sc.add_shape(type="line", x0=0, y0=0, x1=max_cap, y1=max_cap, line=dict(dash="dash"))
                        fig_sc.update_layout(template="plotly_dark", height=360,
                                             xaxis_title="LTV Cap", yaxis_title="LTV (AI)")
                        st.plotly_chart(fig_sc, use_container_width=True)
                    except Exception:
                        pass
            with r2c2:
                value_col = "realizable_value" if "realizable_value" in ft.columns else ("ai_adjusted" if "ai_adjusted" in ft.columns else None)
                if value_col and "city" in ft.columns:
                    try:
                        top_geo = (ft.groupby("city")[value_col].sum()
                                   .sort_values(ascending=False).head(5).reset_index())
                        fig_geo = px.pie(top_geo, values=value_col, names="city", title="Top-5 City Concentration")
                        fig_geo.update_layout(template="plotly_dark", height=360)
                        st.plotly_chart(fig_geo, use_container_width=True)
                    except Exception:
                        pass

            # Row 3: Condition Ã— Legal heatmap
            if {"condition_score","legal_penalty"}.issubset(ft.columns):
                try:
                    cond_bins  = pd.cut(ft["condition_score"], bins=[0,0.70,0.85,1.00], labels=["<0.70","0.70â€“0.85",">0.85"])
                    legal_bins = pd.cut(ft["legal_penalty"],  bins=[0,0.97,0.99,1.00], labels=["<0.97","0.97â€“0.99",">=0.99"])
                    heat = (ft.assign(cond=cond_bins, legal=legal_bins)
                            .groupby(["cond","legal"]).size().reset_index(name="count"))
                    fig_hm = px.density_heatmap(heat, x="legal", y="cond", z="count", title="Condition vs Legal â€” Density")
                    fig_hm.update_layout(template="plotly_dark", height=360)
                    st.plotly_chart(fig_hm, use_container_width=True)
                except Exception:
                    pass

    # ========== 4) HUMAN REVIEW ==========
    with tab4:
        st.subheader("ğŸ§‘â€âš–ï¸ Stage 4 â€” Human Review & Agreement Score")
        src_choice = st.radio("Use AI output from Stage 3, or import a CSV:", ["Use Stage 3 output","Import CSV"])
        df_rev = None
        if src_choice == "Use Stage 3 output":
            df_rev = ss.get("asset_ai_df")
            if df_rev is None:
                st.warning("No Stage 3 output found. Run appraisal first or import a CSV.")
        else:
            up_rev = st.file_uploader("Upload AI decisions CSV", type=["csv"], key="rev_csv")
            if up_rev is not None:
                df_rev = pd.read_csv(up_rev)

        if df_rev is not None:
            if "human_decision" not in df_rev.columns:
                df_rev["human_decision"] = df_rev.get("decision", "approved")
            if "human_reason" not in df_rev.columns:
                df_rev["human_reason"] = ""

            st.markdown("**1) Select rows to review and correct**")
            edited = st.data_editor(df_rev, use_container_width=True, key="human_editor")

            st.markdown("**2) Compute agreement score**")
            if st.button("Compute agreement score", key="btn_agree_score"):
                ai_col = "decision" if "decision" in edited.columns else "rule_decision"
                ai_vals = edited[ai_col].astype(str).str.lower()
                human_vals = edited["human_decision"].astype(str).str.lower()
                agree = (ai_vals == human_vals)
                agree_pct = float(agree.mean() * 100)
                gauge = go.Figure(go.Indicator(
                    mode="gauge+number", value=agree_pct,
                    title={'text': "AI â†” Human Agreement"},
                    gauge={'axis': {'range': [0, 100]}, 'bar': {'color': '#22d3ee'},
                           'steps': [{'range': [0, 70], 'color': '#1e293b'},
                                     {'range': [70, 90], 'color': '#0ea5e9'},
                                     {'range': [90, 100], 'color': '#22d3ee'}]}
                ))
                gauge.update_layout(template="plotly_dark", height=260)
                st.plotly_chart(gauge, use_container_width=True)

                dis = edited.loc[~agree, [c for c in edited.columns if c in ["application_id","asset_id","decision","human_decision","ai_reasons","rule_reasons","human_reason"]]]
                st.markdown(f"âŒ **{len(dis)}** rows disagreed out of **{len(edited)}**  ({(1-agree.mean())*100:.1f}% disagreement rate)")
                st.dataframe(dis, use_container_width=True)

                fname = f"assetappraisal.{ss['asset_user']['name']}.production.{datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S')}.csv"
                st.download_button("â¬‡ï¸ Export review CSV (for Training tab)",
                                   data=edited.to_csv(index=False).encode("utf-8"),
                                   file_name=fname, mime="text/csv")

                ss["asset_human_feedback_df"] = edited

    # ========== 5) TRAINING (Feedback â†’ Retrain) ==========
    with tab5:
        st.subheader("ğŸ§ª Stage 5 â€” Train from Feedback & Promote to PRODUCTION")

        staged = []
        if "asset_human_feedback_df" in ss and ss["asset_human_feedback_df"] is not None:
            buf = ss["asset_human_feedback_df"].to_csv(index=False).encode("utf-8")
            staged.append(("from_stage4.csv", buf))

        up_fb = st.file_uploader("Upload feedback CSV(s)", type=["csv"], accept_multiple_files=True, key="fb_csvs")
        if up_fb:
            for f in up_fb:
                staged.append((f.name, f.getvalue()))

        if staged:
            st.success(f"Staged {len(staged)} feedback file(s) for training.")
            st.json([name for name, _ in staged])

            meta = {
                "user_name": ss["asset_user"]["name"],
                "agent_name": "asset_appraisal",
                "algo_name": "asset_lr"  # adjust to your actual backend algo id
            }
            st.markdown("**Launch Retrain â€” payload preview**")
            st.code(json.dumps(meta, indent=2))

            if st.button("ğŸš€ Train candidate model", key="btn_train_candidate"):
                files = [("files", (name, io.BytesIO(content), "text/csv")) for name, content in staged]
                data = {"meta": json.dumps(meta)}
                job = None
                for agent_id in discover_asset_agents():
                    try:
                        resp = requests.post(f"{API_URL}/v1/agents/{agent_id}/training/train_asset",
                                             files=files, data=data, timeout=180)
                        if resp.ok:
                            job = resp.json(); break
                        else:
                            st.error(f"[{agent_id}] {resp.status_code} {resp.reason}")
                            try:
                                st.code(resp.json())
                            except Exception:
                                st.code(resp.text[:2000])

                    except Exception:
                        pass
                if job is None:
                    st.error("Training endpoint failed on all discovered agent ids.")
                else:
                    st.success("Training job submitted.")
                    st.json(job)

            if st.button("ğŸ“ˆ Promote last candidate to PRODUCTION", key="btn_promote_prod"):
                promoted = None
                for agent_id in discover_asset_agents():
                    try:
                        r = requests.post(f"{API_URL}/v1/agents/{agent_id}/training/promote_last", timeout=60)
                        if r.ok:
                            promoted = r.json(); break
                    except Exception:
                        pass
                if promoted:
                    st.success("Model promoted.")
                    st.json(promoted)
                else:
                    st.error("Promotion failed on all discovered agent ids.")
        else:
            st.info("Drop at least one feedback CSV here or generate from Stage 4.")

